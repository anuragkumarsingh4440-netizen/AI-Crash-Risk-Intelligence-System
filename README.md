# ðŸš¨ AI Crash Risk Intelligence System  (CAPSTONE)
### End-to-End Data Science, Risk Modeling & Decision Intelligence Platform

ðŸ”— **Live Application (Streamlit):** <https://crash-risk-system.streamlit.app/>  
ðŸ“Š **Live Tableau Dashboard:** <(https://public.tableau.com/app/profile/anurag.kumar.singh7896/viz/CAPSTONE_TABLEAU_CRASH/Dashboard1?publish=yes
)>  
ðŸ“¹ **System Demo Video:** <ADD_VIDEO_LINK>  
ðŸ“§ **Contact:** anuragkumarsingh4440@gmail.com  

---

## ðŸ‘¤ Ownership, Role & Development Approach

This project is **fully designed, owned, and led by me as a Data Scientist**.  
I am responsible for:

- Problem formulation
- Data cleaning strategy
- Statistical & probabilistic analysis
- Feature engineering
- Model selection & validation
- Risk scoring logic
- Explainability framework
- Business & policy alignment

### Application Layer Disclosure (Transparent & Professional)

The **Streamlit application (`app.py`) was developed with the help of ChatGPT as an engineering agent**.

This decision was intentional:
- My primary expertise is **data science, modeling, and analytics**
- The application layer is **large-scale**, multi-model, deployment-oriented
- AI was used as an **accelerator**, similar to collaborating with a platform engineer

ðŸ‘‰ **All analytical logic, modeling decisions, risk definitions, and outputs are entirely mine.**
AI-Crash-Risk-Intelligence-System/
â”‚
â”œâ”€â”€ capstone_archiev/
â”‚   â”‚
â”‚   â”œâ”€â”€ 00_dataset/
â”‚   â”‚   â””â”€â”€ Raw crash datasets (original, untouched)
â”‚   â”‚
â”‚   â”œâ”€â”€ 01_DATA_LOAD/
â”‚   â”‚   â””â”€â”€ Data loading, schema checks, data consistency validation
â”‚   â”‚
â”‚   â”œâ”€â”€ 02_EDA_BEFORE_STATS/
â”‚   â”‚   â””â”€â”€ Initial exploratory data analysis
â”‚   â”‚       (distributions, anomalies, missing patterns)
â”‚   â”‚
â”‚   â”œâ”€â”€ 04_INFERENTIAL_STATS/
â”‚   â”‚   â””â”€â”€ Statistical testing and validation
â”‚   â”‚       (hypothesis testing, significance checks)
â”‚   â”‚
â”‚   â”œâ”€â”€ 05_EDA_AFTER_STATS/
â”‚   â”‚   â””â”€â”€ Post-cleaning EDA
â”‚   â”‚       (validated distributions, corrected patterns)
â”‚   â”‚
â”‚   â”œâ”€â”€ 06_MODELS/
â”‚   â”‚   â””â”€â”€ Model experimentation notebooks
â”‚   â”‚       (feature selection, tuning, evaluation)
â”‚   â”‚
â”‚   â””â”€â”€ CAPSTONE.html
â”‚       â””â”€â”€ Final consolidated analysis report (HTML)
â”‚
â”œâ”€â”€ model_registry/
â”‚   â”‚
â”‚   â”œâ”€â”€ model1/final_model_v1/
â”‚   â”‚   â”œâ”€â”€ catboost_injury_severity.pkl
â”‚   â”‚   â””â”€â”€ feature_columns.json
â”‚   â”‚
â”‚   â”œâ”€â”€ model2/final_model_risk_2/
â”‚   â”‚   â”œâ”€â”€ catboost_crash_risk.pkl
â”‚   â”‚   â””â”€â”€ feature_columns.json
â”‚   â”‚
â”‚   â”œâ”€â”€ model3/Model3_HotspotClustering/
â”‚   â”‚   â””â”€â”€ hotspot clustering artifacts
â”‚   â”‚
â”‚   â”œâ”€â”€ model4/final_model_driver_at_fault/
â”‚   â”‚   â”œâ”€â”€ driver_at_fault_model.pkl
â”‚   â”‚   â””â”€â”€ feature_columns.json
â”‚   â”‚
â”‚   â”œâ”€â”€ model5/05_Damage_Model_Tuned/
â”‚   â”‚   â”œâ”€â”€ final_3class_xgboost_model.pkl
â”‚   â”‚   â””â”€â”€ feature_columns.json
â”‚   â”‚
â”‚   â”œâ”€â”€ model6/final_model_driver_distraction/
â”‚   â”‚   â”œâ”€â”€ catboost_driver_distraction.pkl
â”‚   â”‚   â””â”€â”€ feature_columns.json
â”‚   â”‚
â”‚   â””â”€â”€ info.ipynb
â”‚       â””â”€â”€ Model registry documentation & validation notes
â”‚
â”œâ”€â”€ sample_dataset/
â”‚   â””â”€â”€ Lightweight dataset for demo & testing
â”‚
â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Streamlit application
â”‚       (end-to-end inference, risk scoring, explainability, exports)
â”‚
â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Python dependencies for reproducible deployment
â”‚
â”œâ”€â”€ runtime.txt
â”‚   â””â”€â”€ Deployment runtime configuration
â”‚
â”œâ”€â”€ README.md
â”‚   â””â”€â”€ Project overview, usage, and navigation guide
â”‚
â”œâ”€â”€ .gitignore
â”‚   â””â”€â”€ Repository hygiene and data protection
â”‚
â””â”€â”€ .gitattributes
    â””â”€â”€ Git LFS tracking for large model files
![Uploading image.pngâ€¦]()

This reflects **real-world industry practice**, not dependency.

---

## ðŸŽ¯ Problem Statement

Road crash data exists in abundance, but authorities struggle with:

- Reactive accident response
- Manual hotspot identification
- No behavioral attribution
- No severity forecasting
- No explainability
- Disconnected analysis tools

**Goal:**  
Transform raw crash records into **proactive, explainable, operational intelligence** that helps authorities **prevent crashes before they happen**.

---

## ðŸ§  What This System Delivers

- Multi-model crash risk prediction
- Injury & damage severity estimation
- Driver fault & distraction analysis
- Spatial hotspot intelligence
- Composite police risk scoring
- Explainable AI reasoning
- Future high-risk location forecasting
- Executive-ready PDF & CSV outputs

This is **not a dashboard**.  
This is a **decision intelligence system**.

---

## ðŸ§± Complete Data Science Workflow (Strictly Followed)

Below is the **exact end-to-end workflow** followed **before any model training**.

---

### 1. Data Ingestion & Validation
- Safe CSV loading
- Schema enforcement
- Column consistency checks
- No silent failures

---

### 2. Initial Data Understanding
- Structural analysis (`shape`, `info`)
- Feature type assessment
- Domain sanity checks

---

### 3. Duplicate & Record Integrity Analysis
- Row-level duplicates
- Logical duplicates
- Temporal duplication patterns

---

### 4. Anomaly Detection & Data Sanitization
Instead of naÃ¯ve cleaning, the following were handled:

- Misclassified categories
- Junk tokens (`?`, `--`, `/`, mixed labels)
- Inconsistent categorical semantics

All anomalies were **converted to true null states**, not forced values.

---

### 5. Advanced Missing Value Strategy (NO Mean/Median/Mode Bias)

âŒ Mean / Median / Mode were **NOT** the primary strategy.

âœ… Instead, the following were used:

- **Probabilistic frequency-based imputation**
- **Context-aware logical imputation**
- **Temporal forward/backward reasoning**
- **Cross-feature dependency filling**
- **NLP-inspired normalization for text categories**
- Distribution-preserving fills (no distortion)

Rules:
- Columns with >40% nulls â†’ dropped
- Rows dropped only if <5% total loss

---

### 6. Outlier Handling (Conservative & Risk-Safe)
- IQR & Z-score used for detection
- Outliers removed **only if <5%**
- Extreme values preserved when they represented **real crash risk**

---

### 7. Feature Engineering
- Temporal extraction (year, month, week, hour, weekday)
- Behavioral indicators
- Spatial readiness for clustering
- Zero target leakage

---

### 8. Statistical Understanding
- 5-point summary
- Distribution shape analysis
- Skewness & kurtosis interpretation
- Data behavior understood **before modeling**

---

### 9. Deep Exploratory Data Analysis

#### Univariate
- Numeric: distribution, tails, density
- Categorical: dominance, rarity, imbalance

#### Bivariate
- Numericâ€“Numeric correlations
- Categoricalâ€“Numeric impact analysis
- Categoricalâ€“Categorical interaction patterns

#### Multivariate
- Heatmaps
- Interaction plots
- Risk pattern convergence

---

### 10. Inferential Statistics
- Hypothesis-driven checks
- Pattern validation
- Insights documented **before training**

---

### 11. Encoding Strategy (Model-Specific)
- Ordinal encoding (true order only)
- Label encoding (controlled)
- One-hot (N-1 rule)
- Frequency encoding for high-cardinality features

---

### 12. Transformation & Scaling
- Applied **only when statistically justified**
- Box-Cox / Yeo-Johnson
- Robust, Standard, MinMax scaling aligned with models

---

## ðŸ¤– Models Used (Final Production Models)

| Model | Objective | Algorithm |
|------|----------|----------|
Injury Severity | Predict life impact | CatBoost (Multiclass) |
Crash Risk Score | Probability of severe crash | CatBoost (Probabilistic) |
Driver At Fault | Behavioral culpability | CatBoost (Binary) |
Vehicle Damage Extent | Impact intensity | XGBoost (3-Class) |
Driver Distraction Cause | Behavioral cause | CatBoost (Multiclass) |
Crash Hotspots | Spatial clustering | KMeans |

### Why Tree-Based Models?
- Tabular dominance
- Handles non-linearity
- Robust to noise
- Interpretable
- Industry-proven for risk systems

---

## ðŸš¨ Unified Police Risk Scoring

A composite **POLICE_RISK_SCORE** is generated using:

- Injury severity weight
- Damage severity weight
- Driver fault weight
- Crash risk probability bucket

This produces:
- LOW / MEDIUM / HIGH / CRITICAL categories
- Ranked incidents
- Action prioritization

---

## ðŸ§  Explainable AI Layer

For every high-risk case, the system generates:

- Why this case is risky
- Which factors contributed
- What action is recommended

Human language.  
No ML jargon.  
No black box.

---

## ðŸ—ºï¸ Spatial Intelligence

- Interactive dark-theme maps
- Risk-colored points
- Clean, readable popups
- Hotspot cluster summaries
- Coordinate-level clarity

---

## ðŸ”® Next-Month Risk Estimation

Based on:
- Recent crash density
- Severity dominance
- Risk score trends

Authorities receive a **future risk location list** for preventive action.

---

## ðŸ“Š Visualization Strategy

### Tableau
- Instant descriptive analysis
- Historical pattern exploration
- Management-friendly views

### Streamlit Application
- Predictive intelligence
- Risk scoring
- Explainability
- Downloads (CSV & PDF)
- Deployment-ready system

---

## ðŸ“¦ Outputs

- **Human-readable CSV**
- **Executive PDF report**
- **Interactive risk maps**
- **Policy-ready summaries**

---

## ðŸ›¡ï¸ Governance & Ethics

- Decision support only
- No automated enforcement
- Transparent logic
- Audit-ready
- No personal data storage

---

## ðŸ§  Final Note

This project demonstrates **full data science ownership**, not isolated modeling.

Every decision â€” from anomaly handling to deployment â€” was made with:
- Real-world constraints
- Safety implications
- Interpretability
- Operational usability

ðŸ“§ **Contact:** anuragkumarsingh4440@gmail.com
